{
 "metadata": {
  "name": "",
  "signature": "sha256:dfd10e22b426ed803dad4086cc9cb057fbb4a0d47d91ca39b40bfdedee567e3e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext watermark"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%watermark -d -v -p scikit-learn,matplotlib,numpy,pandas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "02/09/2014 \n",
        "\n",
        "CPython 3.4.1\n",
        "IPython 2.0.0\n",
        "\n",
        "scikit-learn 0.15.1\n",
        "matplotlib 1.3.1\n",
        "numpy 1.8.2\n",
        "pandas 0.14.0\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font size=\"1.5em\">[More information](https://github.com/rasbt/watermark) about the `watermark` magic command extension.</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<hr>\n",
      "I would be happy to hear your comments and suggestions. \n",
      "Please feel free to drop me a note via\n",
      "[twitter](https://twitter.com/rasbt), [email](mailto:se.raschka@gmail.com), or [google+](https://plus.google.com/+SebastianRaschka).\n",
      "<hr>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Naive Bayes classifiers in detail"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "- Training a model for spam filtering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Sections"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- [Introduction](#Introduction)\n",
      "- [Naive Bayes classifier](#Naive-Bayes-classifier)\n",
      "    - [Overview](#Overview)\n",
      "    - [Posterior probabilities](#Posterior-probabilities)\n",
      "    - [Class-conditional probabilities](#Class-conditional-probabilities)\n",
      "    - [Prior probabilities](#Prior-probabilities)\n",
      "    - [Evidence](#Evidence)\n",
      "    - [Multinomial Naive Bayes - A toy example](#Multinomial-naive-Bayes-A-toy-example)\n",
      "        - [Additive smoothening](#Additive-smoothening)\n",
      "    - [Naive Bayes and text classfication](#Naive-Bayes-and-text-classfication)\n",
      "        - [Mutli-variate Bernoulli naive Bayes](#Mutli-variate-Bernoulli-naive-Bayes)\n",
      "        - [Multinomial naive Bayes](#Multinomial-naive-Bayes)\n",
      "    - [Variants of the naive Bayes model](#Variants-of-the-naive-Bayes-model)\n",
      "    - [Eager and lazy learning algorithms](#Eager-and-lazy-learning-algorithms)\n",
      "- [Application - The SMS spam dataset](#Application-The-SMS-spam-dataset)\n",
      "    - [Label Encoder](#Label-encoder)\n",
      "    - [Test and training datasets](#Test-and-training-datasets)\n",
      "    - [Feature extraction: Word counts and Vectorizers](#Feature-extraction:-Word-counts-and-Vectorizers)\n",
      "    - [K-fold cross-validation and evaluation metrics](#K-fold-cross-validation-and-evaluation-metrics)\n",
      "        - [Accuracy](#Accuracy)\n",
      "        - [Confusion matrix](#Confusion-matrix)\n",
      "        - [Precision and recall](#Precision-and-recall)\n",
      "        - [Receiver Operator Characteristics - ROC Curves](Receiver-Operator-Characteristics-ROC-Curves)\n",
      "        - [Hyperparameter tuning and Grid Search](#Hyperparameter-tuning-and-Grid-Search)\n",
      "    - [Training and evaluation](#Training-and-evaluation)\n",
      "- [Further readings](#Further-readings)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Starting more than half a century ago, scientists became very serious about addressing the question: \"Can we build a model that learns from available data and automatically makes the right decisions and predictions?\" Looking back, this sounds almost like a rhetoric question, and the answer can be found in enumerous applications that are emerging from the fields of pattern classification, machine learning, and artificial intelligence.  \n",
      "\n",
      "Data from various sensoring devices, combined with powerful learning algorithms and domain knowledge led to many great inventions that we now take for granted in our everyday life: Internet queries via search engines like Google, text recognition at the post office, barcode scanners at the supermarket, the diagnosis of diseases, speech recognition by Siri or Google Now on our mobile phone, just to name a few.\n",
      "\n",
      "One of the sub-fiels of predictive modeling is supervised pattern classification; supervised pattern classification is the task of training a model based on labeled training data, which then can be used to assign a pre-defined class label to new objects. One example that we will explore throughout this article is spam filtering via naive Bayes classifiers in order to predict whether a new text message can be categorized as spam or not-spam.\n",
      "Naive Bayes classifiers, a family of classifiers that are based on the popular Bayes' probability theorem, are known for creating simple yet well performing models, especially in the fields of document classification and disease prediction.\n",
      "\n",
      "![](../../Images/bayes_smsspam/learning_algorithm_1.png)\n",
      "\n",
      "A more detailed overview of predictive modeling can be found in my previous article [Predictive modeling, supervised machine learning, and pattern classification - the big picture](http://sebastianraschka.com/Articles/2014_intro_supervised_learning.html). \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Naive Bayes classifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Overview"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Naive Bayes classifiers are linear classifiers that are known for being simple yet very efficient. The probablistic model of naive Bayes classifiers is based on Bayes' theorem and the adjective \"naive\" comes from the the assumption that the features in a dataset are mutually independent. In practice, the independence assumption is often violated, but naive Bayes classifiers still tend to perform very well under this unrealistic assumption<sup>1</sup>. Especially for small sample sizes, naive Bayes classifiers can outperform the more powerful alternatives<sup>2</sup>."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Being relatively robust, easy to implement, fast, and accurate, naive Bayes classifiers are used in many different fields. Some examples include the diagnosis of diseases and making decisions about treatment processes<sup>3</sup>, the classification of RNA sequences in taxonomic studies<sup>4</sup>, and spam filtering in e-mail clients<sup>5</sup>.  \n",
      "However, strong violations of the independence assumptions and non-linear classification problems can lead to very poor performances of naive Bayes classifiers.  \n",
      "We have to keep in mind that the type of data and the type problem to be solved dictate which classification model we want to choose. In practice, it is always recommended - if possible - to compare different classification models on the particular dataset and consider the prediction performances as well as computational efficiency. \n",
      "\n",
      "**In the following sections, we will take a closer look at the probability model of the naive Bayes classifier and apply the concept to a simple toy problem. Later, we will use a publicly available SMS (text message) collection to train a naive Bayes classifier in Python that allows us to classify unseen messages as spam or ham.**\n",
      "\n",
      "![](../../Images/bayes_smsspam/linear_vs_nonlinear_problems.png)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<hr>\n",
      "\n",
      "<sup>1</sup> Rish, Irina. 2001. \"[An Empirical Study of the Naive Bayes Classifier.](http://www.citeulike.org/user/JoSeK/article/352583)\" In IJCAI 2001 Workshop on Empirical Methods in Artificial Intelligence, 3:41\u201346. \n",
      "\n",
      "<sup>2</sup> Domingos, Pedro, and Michael Pazzani. 1997. \"[On the Optimality of the Simple Bayesian Classifier under Zero-One Loss.](http://link.springer.com/article/10.1023%2FA%3A1007413511361#page-1)\" Machine Learning 29 (2-3): 103\u201330. doi:10.1023/A:1007413511361.\n",
      "\n",
      "<sup>3</sup> Kazmierska, Joanna, and Julian Malicki. 2008. \"[Application of the Na\u00efve Bayesian Classifier to Optimize Treatment Decisions.](http://www.sciencedirect.com/science/article/pii/S0167814007005221)\" Radiotherapy and Oncology 86 (2): 211\u201316. doi:10.1016/j.radonc.2007.10.019.\n",
      "\n",
      "<sup>4</sup> Wang, Q., G. M. Garrity, J. M. Tiedje, and J. R. Cole. 2007. \"[Naive Bayesian Classifier for Rapid Assignment of rRNA Sequences into the New Bacterial Taxonomy.](http://aem.asm.org/content/73/16/5261.abstract)\" Applied and Environmental Microbiology 73 (16): 5261\u201367. doi:10.1128/AEM.00062-07.\n",
      "\n",
      "<sup>5</sup> Sahami, Mehran, Susan Dumais, David Heckerman, and Eric Horvitz. 1998. \"[A Bayesian Approach to Filtering Junk E-Mail.](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.1254)\" In Learning for Text Categorization: Papers from the 1998 Workshop, 62:98\u2013105.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<hr>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Posterior probabilities"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to understand how naive Bayes classifiers work, we have to briefly recapitulate the concept of Bayesian probabilities.\n",
      "The probability model that was formulated by Thomas Bayes (1701-1761) is quite simple yet powerful; we can write in down in simple words as follows:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{equation} \\text{posterior probability} = \\frac{\\text{conditional probability} \\cdot \\text{prior probibility}}{\\text{evidence}} \\end{equation}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This theorem forms the core of the whole concept of naive Bayes classficiation. If we expressed the posterior probability in the context of a classification problem, our question would be: \"For a particular object, what is the probability that this object belongs to class *i* given its observed feature values?\"\n",
      "\n",
      "A very simple example could be, \"What is the probability that a person has diabetes, given a certain value for a pre-breakfast blood glucose measurement and a certain value for a post-breakfast blood glucose measurement?\"\n",
      "\n",
      "\n",
      "$P(\\text{diabetes} \\mid \\pmb x_i) \\;, \\quad \\pmb x_i = [90 \\text{mg/dl}, 145 \\text{mg/dl}]$\n",
      "\n",
      "We can express it more mathematically as:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{equation} P(\\omega_j \\mid \\pmb x_i) = \\frac{P(\\pmb x_i \\mid \\omega_j) \\cdot P(\\omega_j)}{P(\\pmb x_i)} \\end{equation}\n",
      "\n",
      "where \n",
      "\n",
      "- *$i$ = 1, 2, ..., n* (sample number)\n",
      "- *$j$ = 1, 2, ..., m* (class label)\n",
      "\n",
      "- $\\omega_j$ = class $j$\n",
      "- $\\pmb x_i$ = features of sample $i$ \n",
      "- $P(\\pmb x_i \\mid \\omega_j)$ = probability of observing $\\pmb x_i$ given $\\omega_j$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the naive Bayes probability model we classify objects based on the most probable hypothesis. Typically, we use the so-called Maximum A Posteriori (MAP) estimation to formulate a decision rule given the training data. In other words, we assign an observation $\\pmb x_i$ to the class $\\omega_j$ that has the largest posterior probability:\n",
      "\n",
      "\\begin{equation} \\text{predicted class label} \\leftarrow \\underset{j = 1 ..., m} {\\text{arg max}} \\; P(\\omega_{j} \\mid \\pmb x_i)   \\end{equation}\n",
      "\n",
      "In contrast to a Maximum Likelihood (ML) for parameter estimation, the MAP allows us to incorporate an evidence and prior knowledge in order to make a prediction (more about the terms evidence and prior probability later).\n",
      "\n",
      "To continue with our example above, we can formulate the decision rule based on the posterior probabilities as follows:\n",
      "\n",
      "\\begin{equation} \\text{person has diabetes if} \\\\\n",
      "P(\\text{diabetes} \\mid \\pmb x_i) \\ge P(\\text{not-diabetes} \\mid \\pmb x_i) \\\\\n",
      "\\text{else classify person as healthy}.\\end{equation}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Class-conditional probabilities"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bayes classifiers makes the important assumption that our samples are ***i.i.d.***  \n",
      "The abbreviation \"i.i.d.\" stands for \"independent and identically distributed\" and describes random variables that are independent from one another and are drawn from a similar probability distribution. Independence means that the probability of one observation does not affect the probability of another observation (e.g., time series and network graphs are not independent).  One popular example of \"i.i.d.\" variables would be the tossing of a coin: The first coin flip does not affect the outcome of a second coin flip and so forth. E.g., the probability of a fair coin landing on \"heads\" is always 0.5 no matter how often we'd flip the coin.\n",
      "\n",
      "An additional (\"naive\") assumption is the \"conditional independence\" of the features; this assumptions allows us to estimate the class-conditional probabilities, which are often also called \"likelihoods\", for a particular sample directly from the training data instead of evaluating all possibilities of ***x***.\n",
      "\n",
      "So, given an d-dimensional feature vector ***x***, we can calculate the class conditional probability as follows:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{equation} P(\\pmb x \\mid \\omega_j) = P(x_1 \\mid \\omega_j) \\cdot P(x_2 \\mid \\omega_j) \\cdot \\ldots \\cdot P(x_d \\mid \\omega_j) =  \\prod_{k=1}^{d} P( x_k \\mid \\omega_j) \\end{equation}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Where $P(\\pmb x \\mid \\omega_j)$ simply means: \"How likely is it to observe this particular pattern $\\pmb x$ given that it belongs to class $\\omega_j$?\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And the \"individual\" likelihoods for every feature in the feature vector can be estimated via the Maximum Likelihood Estimate. E.g., in case of categorical data:\n",
      "    \n",
      "\\begin{equation} \\hat{P}(x_i \\mid \\omega_j) = \\frac{N_{i,c}}{N_i}  \\quad (i = (1, ..., d))\\end{equation}\n",
      "\n",
      "- $N_{i,c}$: Count of observing feature $x_i$ in class $\\omega_j$.\n",
      "- $N_ic$: Count of observing feature $x_i$ in all classes.$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To illustrate this concept with an example, let's assume that we have a collection of 500 documents where 100 documents are spam messages. Now, we want to calculate the class-conditional probability for a new message \"Hello World\" given that it is \"spam.\"\n",
      "Here, our pattern consists of two features: \"hello\" and \"world,\" and we can calculate the class-conditional probability as product of the \"probability of encountering 'hello' given the message is spam\" and the \"probability of encountering 'world' given the message is 'spam'.\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{equation} P(\\pmb x=[\\text{'hello', 'world'}] \\mid \\omega=\\text{'spam'}) = P(\\text{'hello'} \\mid \\text{'spam'}) \\cdot P(\\text{'world'} \\mid \\text{'spam'}) \\end{equation}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the training dataset of 500 documents, we can use the Maximum Likelihood Estimate to estimate those probabilities: We'd simply calculate how often the words occur in the spam messages. E.g., "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{equation} \\hat{P}(\\pmb x=[\\text{'hello', 'world'}] \\mid \\omega=\\text{'spam'}) = \\frac{20}{100} \\cdot \\frac{2}{100} = 0.004 \\end{equation}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, recalling the \"naive\" assumption of conditional independence, we notice a problem here. Our assumption is that a particular word does not influence the probabilities of encountering other words in the same document. Taking the two words  \"peanut\" and \"butter\" as an example, it is intuitive to say that this assumption is obviously violated: If a text contains the word \"peanut\" it becomes more likely that it also contains the word \"butter.\"\n",
      "\n",
      "In practice, the conditional independence assumption is indeed often violated, but naive Bayes classifiers are known to perform still well in those cases (Zhang, Harry. 2004. \"[The Optimality of Naive Bayes.](http://www.aaai.org/Papers/FLAIRS/2004/Flairs04-097.pdf)\" AA 1 (2): 3)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Prior probabilities"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In contrast to a frequentist's approach, the concept of Bayesian inference introduces an additional prior probability (short: prior) that can be interpreted as a prior belief of *a priori* knowledge.\n",
      "\n",
      "\\begin{equation} \\text{posterior probability} = \\frac{\\text{conditional probability} \\cdot \\text{prior probibility}}{\\text{evidence}} \\end{equation}\n",
      "\n",
      "In the case of pattern classification, the prior probabilities are also called class priors, or in other words: \"The general probability of encountering a particular class.\" In the case of spam classification, the priors could be formulated as $P(\\text{spam})$=\"the probability that any new message is a spam message\" and $P(\\text{ham})= 1-P(\\text{spam})$.\n",
      "\n",
      "Note that if we would choose uniform priors, the posterior probabilities were be entirely determined by the class-conditional probabilities and the evidence term. And since the evidence term is a constant, the decision rule in this case depends  on the class-conditional probabilities, which is similar to a Maximum Likelihood Estimate.\n",
      "\n",
      "\n",
      "We can obtain this *a priori* knowledge e.g., by consulting a domain expert or estimate it from the training data (assuming that the training data is i.i.d. and a representative sample of the entire population. For example, a Maximum Likelihood Estimate approach would be:\n",
      "\n",
      "\\begin{equation} \\hat{P}(\\omega_j) = \\frac{N_{\\omega_j}}{N_c}  \\end{equation}\n",
      "\n",
      "- $N_{\\omega_j}$: Count of samples from class $\\omega_j$.\n",
      "- $N_c$: Count of all samples.\n",
      "\n",
      "If we'd apply this concept to spam classification, we could write:\n",
      "\n",
      "\n",
      "\\begin{equation}\\hat{P}(\\text{spam}) = \\frac{\\text{# of spam messages in training data}}{\\text{# of all mesages in training data}} \\end{equation}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To illustrate the effect of the prior probabilities on the decision rule, let us take a look at the figure below. Here, we have a 1-dimensional pattern *x* (continuous attribute, plotted as \"x\" symbols) that follows a normal distribution and belongs to one out of two classes (blue and green). The patterns from one class ($\\omega_1=\\text{blue}$) are drawn from a normal distribution that has its mean at x=4 and a standard deviation of 1. The probability distribution of the other class ($\\omega_2=\\text{green}$) is centered at x=10 and also has a standard deviation of 1. The bell-curves denote the probability densities of the samples that were drawn from the two different normal distributions. \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far, we've only been looking at the class conditional probabilities, e.g., we could estimate that  \n",
      "\n",
      "- $P(x=4 \\mid \\omega_1) \\approx 0.4$ or $P(x=10 \\mid \\omega_1) < 0.001$ \n",
      "- $P(x=4 \\mid \\omega_2) < 0.001$ or $P(x=10 \\mid \\omega_2) \\approx 0.4$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, if we chose uniform priors, that is $P(\\omega_1) = P(\\omega_2) = 0.5$, the decision rule would be entirely dependent on those class-conditional probabilities, so that the decision rule would fall directly between the two distributions where $P(x \\mid \\omega_1) = P(x \\mid \\omega_2)$. However, if we chose a prior probability for $P(\\omega_1)$ that is > 0.5, the decision region of class $\\omega_1$ would expand, because the posterior probability for $\\omega_1$ will increase.\n",
      "\n",
      "In context of spam classification we could think of encountering a new message that only contains words that are equally likely to appear in spam or ham messages. In this case, we would base our decision on prior knowledge, e.g., we could assume that a random message is in 9 out of 10 cases not spam and therefore classify the new message as ham."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![prior probabilities affecting the decision boundary](../../Images/bayes_smsspam/effect_priors_1.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code that was used to generate this plot can be found [here](http://nbviewer.ipython.org/github/rasbt/pattern_classification/blob/master/Images/bayes_smsspam/pdfs.ipynb)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Evidence"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After we discussed the class-conditional probability and the prior probability, there is only one term missing in order to compute posterior probility: the \"evidence.\"\n",
      "\n",
      "\\begin{equation} \\text{posterior probability} = \\frac{\\text{conditional probability} \\cdot \\text{prior probibility}}{\\text{evidence}} \\end{equation}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The evidence $P(\\pmb x)$ can be understood as the probability of encountering a particular pattern $\\pmb x$ in general (independent from the class label)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given our more formal definition of posterior probability:\n",
      "\\begin{equation} P(\\omega_j \\mid \\pmb x_i) = \\frac{P(\\pmb x_i \\mid \\omega_j) \\cdot P(\\omega_j)}{P(\\pmb x_i)} \\end{equation}\n",
      "\n",
      "we can calculate the evidence as follows:\n",
      "\n",
      "\\begin{equation}P(\\pmb x_i) = P(\\pmb x_i \\mid \\omega_j) \\cdot P(\\omega_j) \\cdot P(\\pmb x_i \\mid \\omega_j^C) \\cdot P(\\omega_j^C)\\end{equation}\n",
      "\n",
      "where $\\omega_j^C$ stands for \"complement\" and basically translate to \"**not** class $\\omega_j$.\" \n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Although the evidence term is required to accurately calculate the posterior probabilities, we don't need to keep this term in our decision rule. \n",
      "\n",
      "E.g., if our decision rule is\n",
      "\n",
      "\"Classify sample $\\pmb x_i$ as $\\omega_1$ if $P(\\omega_1 \\mid \\pmb x_i) > P(\\omega_2 \\mid \\pmb x_i)$ else classify the sample as $\\omega_2$,\"  \n",
      "\n",
      "we can see that the evidence terms simply cancel each other out:\n",
      "\n",
      "\\begin{equation}  \\frac{P(\\pmb x_i \\mid \\omega_1) \\cdot P(\\omega_1)}{P(\\pmb x_i)} > \\frac{P(\\pmb x_i \\mid \\omega_2) \\cdot P(\\omega_2)}{P(\\pmb x_i)} \\end{equation}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{equation}\\Rightarrow P(\\pmb x_i \\mid \\omega_1) \\cdot P(\\omega_1) > P(\\pmb x_i \\mid \\omega_2) \\cdot P(\\omega_2)\\end{equation}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Multinomial naive Bayes - A toy example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To apply the concepts of posterior probabilities and decision rules in a naive Bayes classifier, let us walk through a simple simple toy example given the following training set:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](../../Images/bayes_smsspam/toy_dataset_1.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We assume that this dataset consists of 12 samples from 2 classes indicated by the symbols \"+\" and \"-\". Each sample is described by two features: A color and a shape. \n",
      "\n",
      "- class labels: $\\omega_j \\in \\{+, -\\}$\n",
      "- feature vector: $\\pmb x_i = [x_{i1} \\; x_{i2}], \\quad x_{i1} \\in \\{ \\text{blue}, \\text{green}, \\text{red}, \\text{yellow} \\}, \\quad x_{i2} \\in \\{\\text{circle}, \\text{square} \\}$\n",
      "\n",
      "\n",
      " "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The task is to classify a new sample (while we pretend that we don't know that its true class label is \"+\"):\n",
      "![](../../Images/bayes_smsspam/toy_dataset_2.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Decision rule"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, let us define the decision rule:\n",
      "\n",
      "Classify sample as \"+\"  if   \n",
      "\n",
      "$P(\\omega=\\text{+} \\mid \\pmb x = \\text{blue, square}) \\geq P(\\omega=\\text{-} \\mid \\pmb x = \\text{blue, square})$  \n",
      "\n",
      "else classify sample as \"-.\"\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Prior probabilities"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Under the assumption that our samples are i.i.d, we can obtain the prior probabilities via the Maximum Likelihood Estimate (i.e., calculate the frequencies of how often each class label is represented in the training dataset):\n",
      "\n",
      "$P(\\text{+}) = \\frac{7}{12} = 0.58$    \n",
      "\n",
      "$P(\\text{-}) = \\frac{5}{12} = 0.42$    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Class-conditional probabilities"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Under the assumption that the features \"color\" and \"shape\" are mutually independent, the class-conditional probabilities can be calculated as a simple product of the individual conditional probabilities."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Via Maximum Likelihood Estimate, e.g., $P(\\text{blue} \\mid \\text{-})$ is simply the frequency of observing a \"blue\" sample among all samples in the training dataset that belong to class \"+.\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$P(\\pmb x \\mid \\text{+}) = P(\\text{blue} \\mid \\text{+}) \\cdot P(\\text{square} \\mid \\text{+}) = \\frac{3}{7} \\cdot \\frac{5}{7} = 0.31$   \n",
      "\n",
      "$P(\\pmb x \\mid \\text{-}) = P(\\text{blue} \\mid \\text{-}) \\cdot P(\\text{square} \\mid \\text{-}) = \\frac{3}{5} \\cdot \\frac{3}{5} = 0.36$  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Posterior probabilities"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we can simply calculate the product of the class-conditional and prior probabilities in order to obtain the posterior probabilities."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$P(\\text{+}\\mid \\pmb x) = P(\\pmb x \\mid \\text{+}) \\cdot P(\\text{+}) = 0.31  \\cdot  0.58 = 0.18$  \n",
      "\n",
      "$P(\\text{-} \\mid \\pmb x) = P(\\pmb x \\mid \\text{-}) \\cdot P(\\text{-}) = 0.36  \\cdot  0.42 = 0.15$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Classify"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this last step, we compare the posterior probabilities and assign the class label based on our decision rule:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\text{If } P(\\text{+} \\mid \\pmb x) \\geq P(\\text{-} \\mid \\pmb x) \\Rightarrow \\text{classify as +} \\text{, else } \\text{classify as -}$  \n",
      "\n",
      "And since \n",
      "\n",
      "$0.18 > 0.15$  \n",
      "\n",
      "$\\text{we classify the sample as } \\text{+}$  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we take a look at the calculation of the posterior probabilties again, we can see in this simple example how the prior probabilities affected our decision rule. If we had equal prior probabilities for both classes, the new pattern would be classified as \"-\" instead of \"+.\" This underlines the importance of having a representative training dataset, and in practice, it is usually recommended to additionally consult a domain expert in order to determine the prior probabilities."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Additive smoothening"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point we might wonder what would happen if we encountered a sample that has the color attribute \"yellow.\" \n",
      "\n",
      "![](../../Images/bayes_smsspam/toy_dataset_3.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The color \"yellow\" does not appear in our training dataset. Thus, the class-conditional probability will become 0. The posterior probabilities will also become 0 as result from multiplying the prior probabilities with the class-conditional probabilities.\n",
      "\n",
      "$P(\\omega_1 \\mid \\pmb x) = 0  \\cdot  0.42 = 0$  \n",
      "\n",
      "$P(\\omega_2 \\mid \\pmb x) = 0  \\cdot  0.58 = 0$  \n",
      "\n",
      "In order to make a decision based on the prior probabilities, an additional smoothening term can be added to the multinomial Bayes model to ensure non-zero class-conditional probabilities for categorical data:\n",
      "\n",
      "\n",
      "\\begin{equation} \\hat{P}(x_i \\mid \\omega_j) = \\frac{N_{i,c}+\\alpha}{N_i + \\alpha \\, d}  \\quad (i = (1, ..., d))\\end{equation}\n",
      "\n",
      "- $N_{i,c}$: Count of observing feature $x_i$ in class $\\omega_j$.\n",
      "- $N_ic$: Count of observing feature $x_i$ in all classes.\n",
      "- $\\alpha$: Parameter for additive smoothening. \n",
      "- $d$: Dimensionality of the feature vector $\\pmb x = [x_1, ..., x_d]$.\n",
      "\n",
      "Two common variants of additive smoothening are the so-called Lidstone smoothening ($\\alpha<1$) or  Laplace smoothening ($\\alpha=1$).\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<hr>\n",
      "everything below here is in progress ... \n",
      "<hr>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Naive Bayes and text classfication"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- training set is tokenized (punctuation and formatting characters are removed), optionally stopwords\n",
      "- building vocabulary set based on the words that are ocurring in the training dataset and associate it with a word count.\n",
      "E.g., {\"able\":120, \"acre\":5, ..., \"zoo\":8} (unigram approach; bag of words)\n",
      "\n",
      "- stop words\n",
      "\n",
      "Aristotle quote: \"Change in all things is sweet\".\n",
      "\n",
      "n-grams, \n",
      "\n",
      "1-gram, unigram: change, in, all, things, is, sweet\n",
      "2-gram, bigram: change in, in all, all things, things is, is sweet\n",
      "trigram: change in all, all things is, things is sweet\n",
      "\n",
      "- feature vector is the whole vocabulary for every text (very high dimensional)\n",
      "\n",
      "Maximum Likelihoods estimates for the parameters to calculate the posterior:\n",
      "\n",
      "- prior: $P(\\omega_j)$ simply by frequency, e.g., $\\frac {\\text{# of spam msg.}}{\\text{# of all msg.}}$\n",
      "\n",
      "- class-conditional probability (assuming the words are conditionally independent): \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Mutli-variate Bernoulli naive Bayes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- no term frequencies but Boolean: Does word occur in document? yes or no?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{equation} P(\\pmb x \\mid \\omega_j) = \\prod_{i=1}^{m} P(x_i \\mid \\omega_j)^b \\cdot (1-P(x_i \\mid  \\omega_j))^{(1-b)} \\end{equation}\n",
      "\n",
      "- b = whether word occurs or not $b \\in {0,1}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{equation} \\hat{P}(x_i \\mid \\omega_j) = \\frac{df_{xi, y} + 1}{df_y + 2} \\end{equation} "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- $df_{xi, y}$ is the number of documents in the training dataset that contain $x_i$ and belong to class $\\omega_j$\n",
      "- $df_y$ Number of documents in the training dataset that belong to class $\\omega_j$\n",
      "- +1 and +2 are the parameters for Laplace smoothening\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Multinomial naive Bayes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Term frequency"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A more informative approach to characterize text documents than binary values is to use the so-called \"term frequency\" tf(t, d). The term frequency is typically defined as the number of times a given term *t* (i.e., word) appears in a document *d* -- this approach is sometimes also called \"raw frequency.\" An alternative approach is to normalize the term frequency by dividing the (raw) term frequency by the length of the document.\n",
      "\n",
      "\n",
      "\\begin{equation}\\text{normalized term frequency} = \\frac{tf(t, d)}{n_d}\\end{equation}\n",
      "\n",
      "- $tf(t, d)$: (Raw) term frequency; count of term *t* in document *d*.\n",
      "- $n_d$: Total number of terms in document *d*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use the term frequencies from our training data to estimate the class-conditional probabilities for our multinomial model as follows:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{equation} \\hat{P}(x_i \\mid \\omega_j) = \\frac{\\sum tf(x_i, d \\in \\omega_j) + \\alpha}{\\sum N_{d \\in \\omega j} + \\alpha \\cdot V} \\end{equation} "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- $x_i$: A word from the feature vector $\\pmb x$ for a particular sample.\n",
      "- $\\sum tf(x_i, d \\in \\omega_j)$: The sum of (raw) term frequencies of word $x_i$ from all documents in the training sample that belong to class $\\omega_j$.\n",
      "- $\\sum N_{d \\in \\omega j}$: The sum of all term frequencies in the training dataset for class $\\omega_j$.\n",
      "- $\\alpha$: An addititive smoothening parameter ($\\alpha = 1$ for Laplace smoothening).\n",
      "- $V$: The size of the vocabulary (number of different words in the training set)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And the class-conditional probability of encountering the text $\\pmb x$ can be calculated as the product from the likelihoods of the idndividual words (remember the \"naive\" assumption of conditional independence)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{equation} P(\\pmb x \\mid \\omega_j) = P(x_1 \\mid \\omega_j) \\cdot P(x_2 \\mid \\omega_j) \\cdot   \\dotsc \\cdot P(x_n \\mid \\omega_j) = \\prod_{i= 1}^{m}  P(x_i \\mid \\omega_j)  \\end{equation}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Term frequency - inverse document frequency (Tf-idf)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The \"term frequency - inverse document frequency\" (Tf-idf) is another alternative for characterizing text documents. It can be understood as a weighted \"term frequency\", which is especially useful if stop words weren't removed from the text corpus. The Tf-idf approach assumes that the importance of a word is inversely proportional to how often it occurs across documents. Although Tf-idf is most commonly used to rank documents by relevance in different text mining tasks, such as page ranking by search engines, it can also be applied to text classification via naive Bayes.\n",
      "\n",
      "\\begin{equation}\\text{Tf-idf} = tf_n(t,d) \\times idf(t)\\end{equation}\n",
      "\n",
      "where $tf_n(d,f)$ is the normalized term frequency, and $idf$, the inverse document frequency, is calculated via\n",
      "\n",
      "\\begin{equation}idf(t) = \\log\\Bigg(\\frac{n_d}{n_d(t)}\\Bigg) \\end{equation}\n",
      "\n",
      "- $n_d$: The total number of documents.\n",
      "- $n_d(t)$: The number of documents that contain the term $t$.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Variants of the naive Bayes model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have discussed the \"conditional independence\" assumption in the previous section about the calculation of the class-conditional probabilities. However, we haven't talked about the individual underlying probability distributions of the individual attributes, yet. \n",
      "Basically, there are three different variants of the naive Bayes classifier:\n",
      "\n",
      "- Gaussian naive Bayes\n",
      "- Multinomial naive Bayes\n",
      "- Bernoulli naive Bayes\n",
      "\n",
      "The terms \"Gaussian\", \"Multinomial\", and \"Bernoulli\" just describe how the different features of a one or multi-dimensional dataset are distributed in order to calculate the class-conditional probability $P(\\pmb x_i \\mid \\omega_j)$ for the samples $i= 1, 2, ..., n$ and the different classes $j = 1, 2 ..., m$.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Continuous variables"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The naive Bayes model is not restricted to categorical data like in our example above, but we can also train a naive Bayes classifier on continuous data. The Iris flower data set would be a simple example for a supervised classification task with continuous features: The Iris dataset contains widths and lengths of petals and sepals measured in centimeters.  \n",
      "One strategy for dealing with continuous data in naive Bayes classification would be to discretize the features and form distinct categories. A more common approach is to use a Gaussian or Multinomial kernel to calculate the class-conditional probabilities.\n",
      "\n",
      "For the Iris example, we could assume that the probability distribution for the features  follow a normal  (Gaussian)  distribution:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{equation} P(x_{ik} \\mid \\omega) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\omega}}} \\exp\\left(-\\frac{(x_{ik} - \\mu_{\\omega})^2}{2\\sigma^2_{\\omega}}\\right) \\end{equation}\n",
      "\n",
      "\\begin{equation} P(\\pmb x_i \\mid \\omega) = \\prod_{k=1}^{d} P(\\pmb x_{ik} \\mid \\omega) \\end{equation}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Eager and lazy learning algorithms"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Being an \"eager learner\", one of the advantages of naive classifiers is that they are generally fast in classifying new instances.   \n",
      "The concept of \"eager learners\" describes learning algorithms that learn a model from a training dataset as soon as the data becomes available. Once the model is learned, the training data does not have to be re-evaluated in order to make a new prediction. For \"eager learners\", the computationally most expensive step is the model building, and the classification of new instances is relatively fast.  \n",
      "\"Lazy learners\", however, memorize and re-evaluate the training dataset for prediciting the class label of a new instance. The advantage of \"lazy learning\" is that the model building (training) phase is relatively fast. On the other hand, the acutal prediction is typically slower compared to \"eager learners\" due to the evaluation of the training data. An example of a \"lazy learner\" would be a k-nearest neighbor algorithm. If a new instance is encountered, the algorithm would evaluate the k-nearest neighbors in order to decide upon a class label for the new instance, e.g., via the majority rule (= assign the class label that occurs most frequently amongst the k-nearest neighbors)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Application - The SMS spam dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv(\n",
      "        'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/sms_spam_collection.txt', \n",
      "        sep='\\t', \n",
      "        header=None)\n",
      "\n",
      "df.columns = ['class', 'text']\n",
      "\n",
      "df.head(3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/sebastian/miniconda3/envs/py34/lib/python3.4/site-packages/pandas/io/excel.py:626: UserWarning: Installed openpyxl is not supported at this time. Use >=1.6.1 and <2.0.0.\n",
        "  .format(openpyxl_compat.start_ver, openpyxl_compat.stop_ver))\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>class</th>\n",
        "      <th>text</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> spam</td>\n",
        "      <td> Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>  ham</td>\n",
        "      <td> Go until jurong point, crazy.. Available only ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>  ham</td>\n",
        "      <td>                     Ok lar... Joking wif u oni...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "  class                                               text\n",
        "0  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
        "1   ham  Go until jurong point, crazy.. Available only ...\n",
        "2   ham                      Ok lar... Joking wif u oni..."
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.tail(3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>class</th>\n",
        "      <th>text</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>5569</th>\n",
        "      <td> ham</td>\n",
        "      <td> Pity, * was in mood for that. So...any other s...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5570</th>\n",
        "      <td> ham</td>\n",
        "      <td> The guy did some bitching but I acted like i'd...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5571</th>\n",
        "      <td> ham</td>\n",
        "      <td>                        Rofl. Its true to its name</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "     class                                               text\n",
        "5569   ham  Pity, * was in mood for that. So...any other s...\n",
        "5570   ham  The guy did some bitching but I acted like i'd...\n",
        "5571   ham                         Rofl. Its true to its name"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib import pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "plt.pie(\n",
      "    (len(df[df['class'] == 'spam']), len(df[df['class'] == 'ham'])),\n",
      "    labels=('spam','ham'),\n",
      "    shadow=True,\n",
      "    colors=('yellowgreen', 'lightskyblue'),\n",
      "    explode=(0,0.15),\n",
      "    startangle=90,\n",
      "    autopct='%1.1f%%',\n",
      "    )\n",
      "plt.legend(fancybox=True)\n",
      "plt.axis('equal')\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEjCAYAAABnxZXbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkhSUEEKILkkKSgghRJf0P619h+ouPxedAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x108b18b70>"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Label Encoder"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "X = df['text'].values \n",
      "y = df['class'].values\n",
      "\n",
      "print('before: %s ...' %y[:5])\n",
      "\n",
      "le = LabelEncoder()\n",
      "y = le.fit_transform(y)\n",
      "\n",
      "print('after: %s ...' %y[:5])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "before: ['spam' 'ham' 'ham' 'ham' 'ham'] ...\n",
        "after: [1 0 0 0 0] ...\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Test and training datasets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=12345)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Feature extraction: Word counts and Vectorizers"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
      "\n",
      "vec = CountVectorizer().fit(['Hello World, this is a test!'])\n",
      "data1 = vec.transform(['This is a test'])\n",
      "data2 = vec.transform(['Hello World'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Features:\\n%s' %vec.get_feature_names())\n",
      "print(\"\\nText 'This is a test':\\n%s\" %data1.toarray())\n",
      "print(\"\\nText 'Hello World':\\n%s\" %data2.toarray())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Features:\n",
        "['hello', 'is', 'test', 'this', 'world']\n",
        "\n",
        "Text 'This is a test':\n",
        "[[0 1 1 1 0]]\n",
        "\n",
        "Text 'Hello World':\n",
        "[[1 0 0 0 1]]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\"Stop Words are words which do not contain important significance to be used in Search Queries. Usually these words are filtered out from search queries because they return vast amount of unnecessary information. A better definition is provided below:\n",
      "\n",
      "\u201cWords that do not appear in the index in a particular database because they are either insignificant (i.e., articles, prepositions) or so common that the results would be higher than the system can handle (as in the case of IUCAT where terms such as United States or Department are stop words in keyword searching.) Stop words vary from system to system. Also, some systems will merely ignore stop words where use of stop words in other systems will result in retrieving zero hits. \u201d\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
      "\n",
      "vec = CountVectorizer().fit(X)\n",
      "allwords = vec.transform(X)\n",
      "spam = vec.transform(X[y==1])\n",
      "ham = vec.transform(X[y==0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spam_col_sum = np.sum(spam.toarray(), axis=0)\n",
      "spam_count_sorted = sorted([(count,index) for (index,count) in enumerate(spam_col_sum)], reverse=True)\n",
      "\n",
      "ham_col_sum = np.sum(ham.toarray(), axis=0)\n",
      "ham_count_sorted = sorted([(count,index) for (index,count) in enumerate(ham_col_sum)], reverse=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(8,4))\n",
      "\n",
      "spam_labels = [vec.get_feature_names()[i[1]] for i in spam_count_sorted[:10]]\n",
      "ham_labels = [vec.get_feature_names()[i[1]] for i in ham_count_sorted[:10]]\n",
      "\n",
      "# plot bars\n",
      "x_pos = list(range(len(spam_labels)))\n",
      "\n",
      "ax1.bar(x_pos, \n",
      "        [i[0]/np.sum(spam_col_sum) for i in spam_count_sorted[:10]], \n",
      "        align='center', \n",
      "        alpha=0.5)\n",
      "\n",
      "ax2.bar(x_pos, \n",
      "        [i[0]/np.sum(ham_col_sum) for i in ham_count_sorted[:10]], \n",
      "        align='center', \n",
      "        alpha=0.5)\n",
      "\n",
      "# set axes labels and title\n",
      "ax1.set_ylabel('frequency in total spam text corpus')\n",
      "ax1.set_xticklabels(x_pos, spam_labels)\n",
      "\n",
      "ax2.set_xticklabels(x_pos, ham_labels)\n",
      "ax2.set_ylabel('frequency in total ham text corpus')\n",
      "\n",
      "for ax in (ax1,ax2):\n",
      "    # hiding axis ticks\n",
      "    ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n",
      "            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
      "\n",
      "    # remove axis spines\n",
      "    ax.spines[\"top\"].set_visible(False)  \n",
      "    ax.spines[\"right\"].set_visible(False) \n",
      "    ax.spines[\"bottom\"].set_visible(False) \n",
      "    ax.spines[\"left\"].set_visible(False)\n",
      "\n",
      "plt.tight_layout()\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/sebastian/miniconda3/envs/py34/lib/python3.4/site-packages/matplotlib/backends/backend_agg.py:517: DeprecationWarning: npy_PyFile_Dup is deprecated, use npy_PyFile_Dup2\n",
        "  filename_or_obj, self.figure.dpi)\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAEaCAYAAADzO0ZoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naGrO5hJSHdUAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10a7ff400>"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Top 10 spam words:\\n')\n",
      "for i in spam_count_sorted[:10]:\n",
      "    print('%s (%sx)' %(vec.get_feature_names()[i[1]], i[0]))\n",
      "    \n",
      "print('\\nTop 10 ham words:\\n')\n",
      "for i in ham_count_sorted[:10]:\n",
      "    print('%s (%sx)' %(vec.get_feature_names()[i[1]], i[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top 10 spam words:\n",
        "\n",
        "to (691x)\n",
        "call (355x)\n",
        "you (297x)\n",
        "your (264x)\n",
        "free (224x)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "the (206x)\n",
        "for (204x)\n",
        "now (199x)\n",
        "or (188x)\n",
        "txt (163x)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Top 10 ham words:\n",
        "\n",
        "you (1948x)\n",
        "to (1562x)\n",
        "the (1133x)\n",
        "and (858x)\n",
        "in (823x)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "me (777x)\n",
        "my (754x)\n",
        "is (739x)\n",
        "it (718x)\n",
        "that (560x)\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\"TfidfVectorizer works like the CountVectorizer, but with a more advanced calculation called Term Frequency Inverse Document Frequency (TF-IDF). This is a statistic for measuring the importance of a word in a document or corpus. Intuitively, it looks for words that are more frequent in the current document, compared with their frequency in the whole corpus of documents. You can see this as a way to normalize the results and avoid words that are too frequent, and thus not useful to characterize the instances.\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\"CountVectorizer basically creates a dictionary of words from the text corpus. Then, each instance is converted to a vector of numeric features where each element will be the count of the number of times a particular word appears in the document.\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "K-fold cross-validation and evaluation metrics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The model is most commonly evaluated by the empirical error rate, which is calculated by the number of correct classifications (i.e., false postives and false negatives) divided by the number of total classifications. Inversly, the accuracy of a model is calculated by true postives and true negatives / number of total classifications or simply 1-error."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is important to note that the test set should only be used once for evaluating the prediction error of a classifier after training it on the training dataset; repetitive evaluations of different models using the same test dataset would be prone to overfitting.\n",
      "A common approach to compare and evaluate different pre-processing techniques and models is cross-validation. Here, we will use a variant called \"k-fold cross-validation\" in order to compare different preprocessing steps and demonstrate the usefulness of a Pipeline.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "\n",
      "from sklearn.pipeline import Pipeline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Multinomial Model\n",
      "\n",
      "clf_1 = Pipeline([\n",
      "    ('vectorizer', CountVectorizer()),\n",
      "    ('classifier', MultinomialNB(alpha=1.0, fit_prior=True)),\n",
      "    ])\n",
      "\n",
      "clf_2 = Pipeline([\n",
      "    ('vectorizer', CountVectorizer(ngram_range=(1,2))),\n",
      "    ('classifier', MultinomialNB(alpha=1.0, fit_prior=True)),\n",
      "    ])\n",
      "\n",
      "clf_3 = Pipeline([\n",
      "    ('vectorizer', CountVectorizer(stop_words=\"english\")),\n",
      "    ('classifier', MultinomialNB(alpha=1.0, fit_prior=True)),\n",
      "    ])\n",
      "\n",
      "clf_4 = Pipeline([\n",
      "    ('vectorizer', CountVectorizer(ngram_range=(1,2), stop_words=\"english\")),\n",
      "    ('classifier',  MultinomialNB(alpha=1.0, fit_prior=True)),\n",
      "    ])\n",
      "\n",
      "# Multi-variate Bernoulli\n",
      "\n",
      "clf_5 = Pipeline([\n",
      "    ('vectorizer', CountVectorizer()),\n",
      "    ('classifier', BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True)),\n",
      "    ])\n",
      "\n",
      "clf_6 = Pipeline([\n",
      "    ('vectorizer', CountVectorizer(ngram_range=(1,2))),\n",
      "    ('classifier', BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True)),\n",
      "    ])\n",
      "\n",
      "clf_7 = Pipeline([\n",
      "    ('vectorizer', CountVectorizer(stop_words=\"english\")),\n",
      "    ('classifier', BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True)),\n",
      "    ])\n",
      "\n",
      "clf_8 = Pipeline([\n",
      "    ('vectorizer', CountVectorizer(ngram_range=(1,2), stop_words=\"english\")),\n",
      "    ('classifier', BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True)),\n",
      "    ])\n",
      "\n",
      "# Tf-idf\n",
      "\n",
      "clf_9 = Pipeline([\n",
      "    ('vectorizer', TfidfVectorizer(ngram_range=(1,2), stop_words=\"english\")),\n",
      "    ('classifier',  MultinomialNB(alpha=1.0, fit_prior=True)),\n",
      "    ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score, KFold\n",
      "\n",
      "# Constructing the k-fold cross validation iterator (k=10)  \n",
      "\n",
      "cv = KFold(n=X_train.shape[0],  # total number of samples\n",
      "           n_folds=10,           # number of folds the dataset is divided into\n",
      "           random_state=12345)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Accuracy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "ACC = (TP + TN) / (TP+TN + FP+FN)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "acc_scores = [\n",
      "    cross_val_score(clf, X_train, y_train, cv=cv, scoring='accuracy')\n",
      "            for clf in [clf_1, clf_2, clf_3, clf_4, clf_5, clf_6, clf_7, clf_8]\n",
      "    ]\n",
      "\n",
      "for score,label in zip(acc_scores, \n",
      "                       ['MultinomialNB, CountVectorizer unigram', \n",
      "                        'MultinomialNB, CountVectorizer bigram',\n",
      "                        'MultinomialNB, CountVectorizer unigram w. stopwords', \n",
      "                        'MultinomialNB, CountVectorizer bigram w. stopwords', \n",
      "                        'BernoulliNB, CountVectorizer unigram', \n",
      "                        'BernoulliNB, CountVectorizer bigram',\n",
      "                        'BernoulliNB, CountVectorizer unigram w. stopwords', \n",
      "                        'BernoulliNB, CountVectorizer bigram w. stopwords',\n",
      "                        ]\n",
      "                       ):\n",
      "    print(\"Accuracy: {:.2%} (+/- {:.2%}), {:}\".format(score.mean(), score.std(), label))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 98.46% (+/- 0.77%), MultinomialNB, CountVectorizer unigram\n",
        "Accuracy: 98.44% (+/- 0.67%), MultinomialNB, CountVectorizer bigram\n",
        "Accuracy: 98.49% (+/- 0.57%), MultinomialNB, CountVectorizer unigram w. stopwords\n",
        "Accuracy: 98.56% (+/- 0.77%), MultinomialNB, CountVectorizer bigram w. stopwords\n",
        "Accuracy: 97.41% (+/- 0.95%), BernoulliNB, CountVectorizer unigram\n",
        "Accuracy: 93.15% (+/- 2.08%), BernoulliNB, CountVectorizer bigram\n",
        "Accuracy: 96.95% (+/- 1.13%), BernoulliNB, CountVectorizer unigram w. stopwords\n",
        "Accuracy: 92.74% (+/- 2.19%), BernoulliNB, CountVectorizer bigram w. stopwords\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Confusion matrix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table cellspacing=\"0\" border=\"0\">\n",
      "    <colgroup width=\"60\"></colgroup>\n",
      "    <colgroup span=\"4\" width=\"82\"></colgroup>\n",
      "    <tr>\n",
      "        <td style=\"border-top: 1px solid #c1c1c1; border-bottom: 1px solid #c1c1c1; border-left: 1px solid #c1c1c1; border-right: 1px solid #c1c1c1\" colspan=2 rowspan=2 height=\"44\" align=\"center\" bgcolor=\"#FFFFFF\"><b><font face=\"Helvetica\" size=4><br></font></b></td>\n",
      "        <td style=\"border-top: 1px solid #c1c1c1; border-bottom: 1px solid #c1c1c1; border-left: 1px solid #c1c1c1; border-right: 1px solid #c1c1c1\" colspan=3 align=\"center\" bgcolor=\"#FFFFFF\"><b><font face=\"Helvetica\" size=4>predicted class</font></b></td>\n",
      "        </tr>\n",
      "    <tr>\n",
      "        <td style=\"border-top: 1px solid #c1c1c1; border-bottom: 1px solid #c1c1c1; border-left: 1px solid #c1c1c1; border-right: 1px solid #c1c1c1\" align=\"left\" bgcolor=\"#EEEEEE\"><font face=\"Helvetica\" size=4>0 (=Ham)</font></td>\n",
      "        <td style=\"border-top: 1px solid #c1c1c1; border-bottom: 1px solid #c1c1c1; border-left: 1px solid #c1c1c1; border-right: 1px solid #c1c1c1\" align=\"left\" bgcolor=\"#EEEEEE\"><font face=\"Helvetica\" size=4>1 (=Spam)</font></td>\n",
      "\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <td style=\"border-top: 1px solid #c1c1c1; border-bottom: 1px solid #c1c1c1; border-left: 1px solid #c1c1c1; border-right: 1px solid #c1c1c1\" rowspan=3 height=\"116\" align=\"center\" bgcolor=\"#F6F6F6\"><b><font face=\"Helvetica\" size=4>true class</font></b></td>\n",
      "        <td style=\"border-top: 1px solid #c1c1c1; border-bottom: 1px solid #c1c1c1; border-left: 1px solid #c1c1c1; border-right: 1px solid #c1c1c1\" align=\"left\" bgcolor=\"#EEEEEE\"><font face=\"Helvetica\" size=4>0 (=Ham)</font></td>\n",
      "        <td style=\"border-top: 1px solid #c1c1c1; border-bottom: 1px solid #c1c1c1; border-left: 1px solid #c1c1c1; border-right: 1px solid #c1c1c1\" align=\"left\" bgcolor=\"#99FFCC\"><font face=\"Helvetica\" size=4>True Negative (TN)</font></td>\n",
      "        <td style=\"border-top: 1px solid #c1c1c1; border-bottom: 1px solid #c1c1c1; border-left: 1px solid #c1c1c1; border-right: 1px solid #c1c1c1\" align=\"left\" bgcolor=\"#F6F6F6\"><font face=\"Helvetica\" size=4>False Positive (FP)</font></td>\n",
      "\n",
      "    </tr>\n",
      "    <tr>\n",
      "        <td style=\"border-top: 1px solid #c1c1c1; border-bottom: 1px solid #c1c1c1; border-left: 1px solid #c1c1c1; border-right: 1px solid #c1c1c1\" align=\"left\" bgcolor=\"#EEEEEE\"><font face=\"Helvetica\" size=4>1 (=Spam)</font></td>\n",
      "        <td style=\"border-top: 1px solid #c1c1c1; border-bottom: 1px solid #c1c1c1; border-left: 1px solid #c1c1c1; border-right: 1px solid #c1c1c1\" align=\"left\" bgcolor=\"#FFFFFF\"><font face=\"Helvetica\" size=4>False Negative (FN)</font></td>\n",
      "        <td style=\"border-top: 1px solid #c1c1c1; border-bottom: 1px solid #c1c1c1; border-left: 1px solid #c1c1c1; border-right: 1px solid #c1c1c1\" align=\"left\" bgcolor=\"#99FFCC\"><font face=\"Helvetica\" size=4>True Positive (TP)</font></td>\n",
      "\n",
      "    </tr>\n",
      "\n",
      "</table>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Precision and recall"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compute the precision\n",
      "The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "PREC tp / (tp + fp)\n",
      "\"The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\"\n",
      "\"\"of all things that were labeled as spam, how many were actually spam?\"\"\n",
      "\n",
      "RECALL tp / (tp + fn)\n",
      "\"recall is intuitively the ability of the classifier to find all the positive samples.\"\n",
      "\"\"of all the things that are truly spam, how many did we label?\"\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prec_recall_scores = [\n",
      "    (cross_val_score(clf, X_train, y_train, cv=cv, scoring='precision'),\n",
      "    cross_val_score(clf, X_train, y_train, cv=cv, scoring='recall'))\n",
      "            for clf in [clf_1, clf_2, clf_3, clf_4, clf_5, clf_6]\n",
      "    ]\n",
      "\n",
      "for score,label in zip(prec_recall_scores, \n",
      "                       ['CountVectorizer unigram', \n",
      "                        'CountVectorizer bigram',\n",
      "                        'CountVectorizer trigram', \n",
      "                        'TfidfVectorizer unigram', \n",
      "                        'TfidfVectorizer bigram',\n",
      "                        'TfidfVectorizer trigram'\n",
      "                        ]\n",
      "                       ):\n",
      "    print(\"Precision: {:.2%} (+/- {:.2%}), {:}\".format(score[0].mean(), score[0].std(), label))\n",
      "    print(\"Recall: {:.2%} (+/- {:.2%}), {:}\\n\".format(score[1].mean(), score[1].std(), label))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import GaussianNB\n",
      "\n",
      "clf_7 = Pipeline([\n",
      "    ('vectorizer', CountVectorizer(ngram_range=(1,2), stop_words='english')),\n",
      "    ('classifier', MultinomialNB()),\n",
      "    ])\n",
      "\n",
      "clf_8 = Pipeline([\n",
      "    ('vectorizer', TfidfVectorizer(ngram_range=(1,2), stop_words='english')),\n",
      "    ('classifier', MultinomialNB()),\n",
      "    ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prec_recall_scores = [\n",
      "    (cross_val_score(clf, X_train, y_train, cv=cv, scoring='precision'),\n",
      "     cross_val_score(clf, X_train, y_train, cv=cv, scoring='recall'))\n",
      "            for clf in [clf_2, clf_7, clf_5, clf_8]\n",
      "    ]\n",
      " \n",
      "for score,label in zip(prec_recall_scores, \n",
      "                       [\n",
      "                        'CountVectorizer bigram',\n",
      "                        'CountVectorizer bigram w. stop words',\n",
      "                        'TfidfVectorizer bigram',\n",
      "                        'TfidfVectorizer bigram w. stop words',\n",
      "                        ]\n",
      "                       ):\n",
      "    print(\"Precision: {:.2%} (+/- {:.2%}), {:}\".format(score[0].mean(), score[0].std(), label))\n",
      "    print(\"Recall: {:.2%} (+/- {:.2%}), {:}\\n\".format(score[1].mean(), score[1].std(), label))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Precision: 99.25% (+/- 1.21%), CountVectorizer bigram\n",
        "Recall: 88.99% (+/- 3.90%), CountVectorizer bigram\n",
        "\n",
        "Precision: 97.60% (+/- 1.91%), CountVectorizer bigram w. stop words\n",
        "Recall: 91.48% (+/- 4.24%), CountVectorizer bigram w. stop words\n",
        "\n",
        "Precision: 100.00% (+/- 0.00%), TfidfVectorizer bigram\n",
        "Recall: 55.56% (+/- 6.28%), TfidfVectorizer bigram\n",
        "\n",
        "Precision: 100.00% (+/- 0.00%), TfidfVectorizer bigram w. stop words\n",
        "Recall: 68.58% (+/- 6.02%), TfidfVectorizer bigram w. stop words\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Receiver Operator Characteristics - ROC Curves"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Hyperparameter tuning and Grid Search"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- GridSearch and roc_auc_score\n",
      "- alpha and priors\n",
      "\n",
      "Regarding your actual problem: you can try to adjust the learner parameters with GridSearchCV using scoring='precision' or a custom scoring function based on sklearn.metric.fbeta_score or roc_auc_score. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Training and evaluation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf_2.fit(X_train, y_train)\n",
      "y_pred = clf_2.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "cm = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "fig = plt.figure(figsize=(6,6))\n",
      "ax = plt.subplot(111) \n",
      "\n",
      "cax = ax.matshow(cm)\n",
      "\n",
      "fig.colorbar(cax)\n",
      "\n",
      "ax.set_ylabel('True label')\n",
      "ax.set_xlabel('Predicted label')\n",
      "ax.set_xticklabels(['', 'ham', 'spam'])\n",
      "ax.set_yticklabels(['', 'ham', 'spam'])\n",
      "\n",
      "plt.title('Confusion matrix')\n",
      "plt.show()\n",
      "\n",
      "print(cm)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAFdCAYAAAADjBo0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4oBgZmaAA4KZmSUOCGZmBjggmJlZ8v8B9zKfvM95h94AAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10bb00da0>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1426    6]\n",
        " [  23  217]]\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
      "\n",
      "for score in [accuracy_score, recall_score, precision_score]:\n",
      "    print('%s: %s' %(score.__name__, score(y_test, y_pred)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "accuracy_score: 0.982655502392\n",
        "recall_score: 0.904166666667\n",
        "precision_score: 0.973094170404\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "ROC curve"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import roc_curve, auc\n",
      "probas = clf_2.fit(X_train, y_train).predict_proba(X_test)\n",
      "fpr, tpr, thresholds = roc_curve(y_test, probas[:, 1], pos_label=1)\n",
      "roc_auc = auc(fpr, tpr)\n",
      "\n",
      "plt.xlabel('False Positive Rate')\n",
      "plt.ylabel('True Positive Rate')\n",
      "plt.title('Receiver operating characteristic (ROC)')\n",
      "\n",
      "plt.plot(fpr, tpr, lw=1)\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEZCAYAAACEkhK6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCcPMzEpxwjAzs1KcMMzMrJT/D0EQa2klsbyOAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1089516a0>"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<br>\n",
      "<br>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Further readings"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[[back to top](#Sections)]"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Naive Bayes classifiers"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Domingos, Pedro, and Michael Pazzani. \"[On the Optimality of the Simple Bayesian Classifier under Zero-One Loss.](http://link.springer.com/article/10.1023%2FA%3A1007413511361)\" Machine Learning 29, no. 2\u20133 (November 1, 1997): 103\u201330. doi:10.1023/A:1007413511361.\n",
      "\n",
      "- Rish, Irina. \"[An Empirical Study of the Naive Bayes Classifier.](http://www.citeulike.org/user/JoSeK/article/352583)\" In IJCAI 2001 Workshop on Empirical Methods in Artificial Intelligence, 3:41\u201346, 2001. \n",
      "\n",
      "- Rennie, Jason D., Lawrence Shih, Jaime Teevan, David R. Karger, and others. 2003. \u201c[Tackling the Poor Assumptions of Naive Bayes Text Classifiers.](http://www.aaai.org/Papers/ICML/2003/ICML03-081.pdf)\u201d In ICML, 3:616\u201323. Washington DC).\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Naive Bayes and spam filtering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Sahami, Mehran, Susan Dumais, David Heckerman, and Eric Horvitz. \"[A Bayesian Approach to Filtering Junk E-Mail.](http://research.microsoft.com/en-us/um/people/horvitz/junkfilter.htm)\" In Learning for Text Categorization: Papers from the 1998 Workshop, 62:98\u2013105, 1998. \n",
      "\n",
      "- Almeida, Tiago A., Jos\u00e9 Mar\u00eda G. Hidalgo, and Akebo Yamakami. \"[Contributions to the Study of SMS Spam Filtering: New Collection and Results.](http://dl.acm.org/citation.cfm?doid=2034691.2034742)\" In Proceedings of the 11th ACM Symposium on Document Engineering, 259\u201362. DocEng \u201911. New York, NY, USA: ACM, 2011. doi:10.1145/2034691.2034742.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}